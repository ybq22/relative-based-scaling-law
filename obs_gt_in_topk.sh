#!/usr/bin/env bash

# ========= åŸºç¡€è®¾ç½® =========
GPUS=(0 1 2 3 5)  # å¯æ ¹æ®éœ€è¦ä¿®æ”¹
MULTI_GPU_DEVICES="0,1,2,3,5"
N_GPUS=${#GPUS[@]}

data_num=1000
export HF_HOME="/data-share/guest/yuebaoqing/.hf_cache"
export HF_ENDPOINT="https://hf-mirror.com"

TASK_FILE="/tmp/obs_gt_in_topk_queue.txt"
LOCK_FILE="/tmp/obs_gt_in_topk_task.lock"

# ========== ä¸­æ–­å¤„ç† ==========
cleanup() {
    echo "âš ï¸ ä¸­æ–­ä¿¡å·æ”¶åˆ°ï¼Œç»ˆæ­¢æ‰€æœ‰å­è¿›ç¨‹..."
    pkill -P $$
    rm -f "$TASK_FILE" "$LOCK_FILE"
    exit 1
}
trap cleanup SIGINT SIGTERM

# ========== æ¨¡å‹åˆ—è¡¨ ==========
eval_model_names=(
    "EleutherAI/pythia-14m"
    "EleutherAI/pythia-31m"
    "EleutherAI/pythia-70m"
    "EleutherAI/pythia-160m"
    "EleutherAI/pythia-410m"
    "EleutherAI/pythia-1b"
    "EleutherAI/pythia-1.4b"
    "EleutherAI/pythia-2.8b"
    "EleutherAI/pythia-6.9b"

    "facebook/opt-125m"
    "facebook/opt-350m"
    "facebook/opt-1.3b"
    "facebook/opt-2.7b"
    "facebook/opt-6.7b"

    "openai-community/gpt2"
    "openai-community/gpt2-large"
    "openai-community/gpt2-medium"
    "openai-community/gpt2-xl"

    "Qwen/Qwen2.5-0.5B"
    "Qwen/Qwen2.5-1.5B"
    "Qwen/Qwen2.5-3B"
    "Qwen/Qwen2.5-7B"
)
output_prefix="eval_gt_in_topk"
# ========== æ•°æ®é›†åˆ—è¡¨ ==========
datasets=(
    # "wikimedia/wikipedia"
    # "openai/gsm8k"
    # "openai/openai_humaneval"
    # "hotpotqa/hotpot_qa"
    # "isaacus/open-australian-legal-corpus"
    "monology/pile-uncopyrighted/Github"
    "allenai/c4"
)

# ========== å¤šå¡å¤§æ¨¡å‹åˆ—è¡¨ ==========
multi_gpu_models=("Qwen/Qwen2.5-14B" "EleutherAI/pythia-12b")

get_label_model() {
    model_name=$1
    if [[ $model_name == EleutherAI/* ]]; then
        echo "EleutherAI/pythia-14m"
    elif [[ $model_name == facebook/* ]]; then
        echo "facebook/opt-125m"
    elif [[ $model_name == openai-community/* ]]; then
        echo "openai-community/gpt2-medium"
    elif [[ $model_name == Qwen/* ]]; then
        # Qwen ç³»åˆ—ï¼šlabel model å°±æ˜¯è‡ªå·±
        echo "$model_name"
    else
        echo "æœªçŸ¥ç³»åˆ—: $model_name" >&2
        exit 1
    fi
}

# ========== å†™å…¥ä»»åŠ¡é˜Ÿåˆ—ï¼ˆåªå†™å•å¡å°æ¨¡å‹ï¼‰ ==========
> "$TASK_FILE"
for dataset in "${datasets[@]}"; do
    for eval_model in "${eval_model_names[@]}"; do
        # è·³è¿‡å¤šå¡æ¨¡å‹
        if [[ " ${multi_gpu_models[@]} " =~ " ${eval_model} " ]]; then
            continue
        fi
        echo "${dataset}|${eval_model}" >> "$TASK_FILE"
    done
done

# ========== è·å– batch size ==========
get_bs() {
    case "$1" in
        # Pythia series models
        *pythia-14m) echo 64 ;;
        *pythia-31m) echo 32 ;;
        *pythia-70m) echo 32 ;;
        *pythia-160m) echo 32 ;;
        *pythia-410m) echo 32 ;;
        *pythia-1b) echo 8 ;;
        *pythia-1.4b) echo 4 ;;
        *pythia-2.8b) echo 4 ;;
        *pythia-6.9b) echo 2 ;;
        *pythia-12b) echo 1 ;;
        
        # Facebook/OPT series models
        *opt-125m) echo 64 ;;
        *opt-350m) echo 32 ;;
        *opt-1.3b) echo 8 ;;
        *opt-2.7b) echo 4 ;;
        *opt-6.7b) echo 2 ;;
        *opt-13b) echo 1 ;;

        # OpenAI/GPT-2 series models
        *gpt2) echo 32 ;;
        *gpt2-medium) echo 16 ;;
        *gpt2-large) echo 8 ;;
        *gpt2-xl) echo 4 ;;
        
        # Qwen2.5 series models
        *Qwen2.5-0.5B) echo 16 ;;
        *Qwen2.5-1.5B) echo 8 ;; # Use 16 for Qwen2.5-1.5B based on reported usage
        *Qwen2.5-3B) echo 4 ;;
        *Qwen2.5-7B) echo 1 ;;  # For standard hardware, reduce batch size. Example uses micro-batch 2 for 4 GPUs
        *qwen2.5-14B) echo 1 ;;
        *Qwen2.5-32B) echo 1 ;;
        *Qwen2.5-72B) echo 1 ;; # Likely requires multiple GPUs or deep quantization
        
        # Default case for any other model
        *) echo "âŒ Unknown model: $1" >&2; exit 1 ;;
    esac
}


# ========== è·å–ä»»åŠ¡ ==========
get_next_task() {
    local line=""
    {
        flock -x 200
        line=$(head -n 1 "$TASK_FILE")
        if [[ -n "$line" ]]; then
            tail -n +2 "$TASK_FILE" > "${TASK_FILE}.tmp" && mv "${TASK_FILE}.tmp" "$TASK_FILE"
        fi
        echo "$line"
    } 200>"$LOCK_FILE"
}

# ========== å¤šå¡å¤§æ¨¡å‹å¤„ç† ==========
for model in "${multi_gpu_models[@]}"; do
    for dataset in "${datasets[@]}"; do
        # label_model=$model # for Qwen only
        label_model=$(get_label_model "$model")
        data_dir="gen_dataset/corpus/${dataset}/${label_model}/test"
        output_dir="$output_prefix/${dataset}/${model}"

        echo "ğŸš€ [Multi-GPU] å¼€å§‹ä»»åŠ¡: $model on dataset $dataset"
        export CUDA_VISIBLE_DEVICES=$MULTI_GPU_DEVICES

        python src/obs_gt_in_topk.py \
            --model "$model" \
            --data_dir "$data_dir" \
            --output_dir "$output_dir" \
            --data_num "$data_num"

        echo "âœ… [Multi-GPU] å®Œæˆä»»åŠ¡: $model on dataset $dataset"
    done
done

# ========== å• GPU worker ==========
gpu_worker() {
    local cuda_id=$1
    while true; do
        task=$(get_next_task)
        if [[ -z "$task" ]]; then
            echo "âœ… [GPU $cuda_id] æ— ä»»åŠ¡ï¼Œé€€å‡º"
            break
        fi

        IFS='|' read -r dataset eval_model <<< "$task"
        bs=$(get_bs "$eval_model")
        grad=$((1024 / bs))
        label_model=$(get_label_model "$eval_model")

        data_dir="gen_dataset/corpus/${dataset}/${label_model}/test"
        output_dir="$output_prefix/${dataset}/${eval_model}"

        echo "ğŸš€ [GPU $cuda_id] å¼€å§‹ä»»åŠ¡: $eval_model on dataset $dataset (bs=$bs)"
        export CUDA_VISIBLE_DEVICES="$cuda_id"

        python src/obs_gt_in_topk.py \
            --model "$eval_model" \
            --data_dir "$data_dir" \
            --output_dir "$output_dir" \
            --data_num "$data_num"

        echo "âœ… [GPU $cuda_id] å®Œæˆä»»åŠ¡: $eval_model on dataset $dataset"
    done
}

# ========== å¯åŠ¨å•å¡ GPU worker ==========
for cuda_id in "${GPUS[@]}"; do
    gpu_worker "$cuda_id" &
done

wait
echo "âœ… æ‰€æœ‰å•å¡æ¨¡å‹ä»»åŠ¡å®Œæˆï¼"



# ========== æ¸…ç† ==========
rm -f "$TASK_FILE" "$LOCK_FILE"
echo "ğŸ‰ æ‰€æœ‰ Raw Wiki obs GT ä»»åŠ¡å®Œæˆï¼"
